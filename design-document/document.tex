\documentclass[12pt,a4paper]{article}

% ----- PACKAGES -----
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage[maxbibnames=3,minbibnames=1,maxcitenames=2,mincitenames=1,style=numeric-comp,backend=biber]{biblatex}
\addbibresource{references.bib}


\geometry{margin=1in}
\setstretch{1.2}
\pagestyle{fancy}
\fancyhf{}
\rhead{Project Design Document}
\lhead{\leftmark}
\rfoot{Page \thepage}

\title{
    \vspace{-2.5cm}
    \textbf{Advanced Information Retrieval}\\[0.2cm]
    \large Fine-Tuning and Transferability in Legal Information Retrieval\\[0.2cm]
    \normalsize Group Number: 27\\[0.3cm]
}
\author{
    Raphael Habichler\thanks{student-id: 12419578, Role: Data Processing \& Fine-tuning} \and
    Mark Sesko\thanks{student-id: 12114879, Role: Evaluation \& Analysis} \and
    Paul Brandst√§tter\thanks{student-id: 12212566, Role: Dataset Preparation \& Documentation}
}
\date{\today}

\titleformat{\section}{\large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\normalsize\bfseries}{\thesubsection}{1em}{}

\begin{document}
\maketitle

\section{Abstract}
This project investigates the transferability of domain-specific fine-tuned embedding models across different legal jurisdictions. We fine-tune the Qwen3-Embedding-8B model on German legal data and evaluate its performance on Austrian, German, and Chinese legal corpora to assess cross-jurisdictional knowledge transfer.

\section{Idea and Goal}
\textbf{Research Question}: Can a legal embedding model fine-tuned on German law effectively retrieve relevant documents in other jurisdictions (Austrian and Chinese law)?

The goal is to build legal information retrieval systems that work across multiple jurisdictions without requiring extensive training data for each legal system. This addresses the challenge of cross-jurisdictional legal search and comparative law analysis.

\section{Main Task}
Our focus is on fine-tuning a state-of-the-art embedding model (Qwen3-Embedding-8B \cite{qwen3embedding}) on German legal data and measuring how well this domain-specific knowledge transfers to other legal systems, specifically Austrian and Chinese law.

\section{Dataset and Processing}
\subsection{Data Sources}
\begin{itemize}
    \item \textbf{Austrian Law Data}: Synthetic dataset created from Austrian legal documents. Since no pre-labeled query-document pairs exist, we generate queries using LLMs to create evaluation pairs.
    \item \textbf{German Law Data}: GerDaLIR dataset \cite{wrzalik-krechel-2021-gerdalir} and German legal corpus \cite{9723721} with pre-labeled query-document pairs for fine-tuning and evaluation.
    \item \textbf{Chinese Law Data}: LeCaRDv2 dataset \cite{li2023lecardv2} with existing query-document annotations.
\end{itemize}

\subsection{Processing}
Data preprocessing includes text normalization, document segmentation, and query-document pair preparation. For Austrian data, we synthetically generate queries from legal documents to create the evaluation dataset.

\section{Methods and Models}
We use the Qwen3-Embedding-8B model \cite{qwen3embedding} and compare two experimental conditions:
\begin{enumerate}
    \item \textbf{Baseline}: Pre-trained Qwen3-Embedding-8B evaluated directly on all three jurisdictions
    \item \textbf{Fine-tuned}: Model fine-tuned specifically on German domain-specific dataset, then evaluated on all three jurisdictions to assess transfer learning
\end{enumerate}

\subsection{Fine-tuning Process}
The fine-tuning procedure adapts the pre-trained model to legal domain:
\begin{itemize}
    \item \textbf{Training data}: German legal query-document pairs from GerDaLIR dataset
    \item \textbf{Loss function}: Cosine similarity loss that maximizes similarity between relevant query-document pairs and minimizes similarity for irrelevant pairs
    \item \textbf{Architecture}: Both queries and documents are independently encoded through the embedding model, then pooled to create fixed-size vector representations
    \item \textbf{Objective}: Learn embeddings where semantically related legal texts are closer in vector space
\end{itemize}

This domain-adapted model is then tested on Austrian (synthetic) and Chinese legal data to measure how well legal knowledge transfers across jurisdictions and languages.

\section{Evaluation}
We measure retrieval performance using standard information retrieval metrics:
\begin{itemize}
    \item \textbf{Precision@k}: Proportion of relevant documents in top-k results
    \item \textbf{Recall@k}: Proportion of all relevant documents found in top-k results
    \item \textbf{nDCG@k}: Normalized Discounted Cumulative Gain, accounting for ranking quality
\end{itemize}

Performance is compared between the baseline and fine-tuned models across all three jurisdictions to quantify transferability.

\section{Workflow}
Figure~\ref{fig:flowchart} illustrates our complete research pipeline from data collection through evaluation.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{flowchart.pdf}
\caption{Project workflow and methodology}
\label{fig:flowchart}
\end{figure}

\newpage
\printbibliography 
\end{document}
