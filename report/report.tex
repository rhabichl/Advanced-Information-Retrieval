\documentclass[12pt,a4paper]{article}

% ----- PACKAGES -----
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[maxbibnames=3,minbibnames=1,maxcitenames=2,mincitenames=1,style=numeric-comp,backend=biber]{biblatex}
\addbibresource{references.bib}


\geometry{margin=1in}
\setstretch{1.2}
\pagestyle{fancy}
\fancyhf{}
\rhead{Project Report}
\lhead{\leftmark}
\rfoot{Page \thepage}

\title{
    \textbf{Advanced Information Retrieval}\\[0.2cm]
    \large Fine-Tuning and Transferability in Legal Information Retrieval\\[0.2cm]
    \normalsize Group Number: 27\\[0.3cm]
}
\author{
    Raphael Habichler\thanks{student-id: 12419578, Role: Data Processing \& Fine-tuning} \and
    Mark Sesko\thanks{student-id: 12114879, Role: Evaluation \& Analysis} \and
    Paul Brandst√§tter\thanks{student-id: 12212566, Role: Dataset Preparation \& Documentation}
}
\date{\today}

\titleformat{\section}{\large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\normalsize\bfseries}{\thesubsection}{1em}{}

\begin{document}
\maketitle

\newpage

\section{Introduction}

Legal information retrieval systems face the challenge of working across different jurisdictions and legal systems. Each country has its own legal framework, terminology, and document structure, making it difficult to build universal search systems. Traditional approaches require training separate models for each jurisdiction, which is expensive and time-consuming. This project investigates whether a single embedding model fine-tuned on one legal system can effectively transfer to others.

Our research question is: Can a legal embedding model fine-tuned on German law effectively retrieve relevant documents in other jurisdictions, specifically Austrian and Chinese law? We aim to determine if fine-tuning on German legal data provides sufficient knowledge transfer to enable cross-jurisdictional legal document retrieval without requiring large training datasets for each target jurisdiction.

We use the Qwen3-Embedding-8B model as our base embedding model. This model is a state-of-the-art text embedding model designed for semantic search and information retrieval tasks. We fine-tune this model on German legal query-document pairs and evaluate its performance on three datasets: German, Austrian, and Chinese legal documents. The goal is to measure how well the fine-tuned model transfers across different legal systems and languages.

\section{Related Work}

Legal information retrieval has been an active area of research, with several datasets and benchmarks developed for different languages and jurisdictions. GerDaLIR provides a German dataset for legal information retrieval with over 123,000 queries and 131,000 case documents \cite{wrzalik-krechel-2021-gerdalir}. This dataset has been used to train and evaluate German legal search systems. LeCaRDv2 offers a large-scale Chinese legal case retrieval dataset with existing annotations \cite{li2023lecardv2}, enabling evaluation of Chinese legal retrieval systems.

Recent advances in transformer-based embedding models have shown strong performance in information retrieval tasks. The Qwen3-Embedding-8B model represents a foundation model approach to text embedding, achieving state-of-the-art results on various benchmarks \cite{qwen3embedding}. These models can be fine-tuned on domain-specific data to improve performance in specialized domains like legal text.

Transfer learning has been widely studied in natural language processing, where models pre-trained on large corpora are fine-tuned on specific tasks or domains. However, cross-jurisdictional transfer in legal information retrieval remains less explored. The challenge lies in whether domain-specific fine-tuning on one legal system can generalize to others, given the differences in legal frameworks, languages, and document structures.

\section{Experiments and Results}

\subsection{Experimental Setup}

We conducted experiments using three legal datasets. The German dataset comes from GerDaLIR, which provides pre-labeled query-document pairs for training and testing \cite{wrzalik-krechel-2021-gerdalir}. The Austrian dataset is synthetic, created from Austrian legal documents with queries generated using large language models since no labeled pairs exist. The Chinese dataset uses LeCaRDv2 with existing annotations \cite{li2023lecardv2}.

For fine-tuning, we used the Qwen3-Embedding-8B model with LoRA parameter-efficient fine-tuning adapters. The model was quantized to 8-bit precision to reduce memory requirements. We trained on German legal query-document pairs from GerDaLIR using the CachedMultipleNegativesRankingLoss, which optimizes for cosine similarity between relevant query-document pairs. Training was performed for 5 epochs with a learning rate of 2e-4, using gradient accumulation to simulate larger batch sizes. The fine-tuned model adapters were saved and made available on HuggingFace.

For evaluation, we generated embeddings for queries and documents using both the baseline pre-trained model and the fine-tuned model. Documents were truncated to 1024 characters to ensure consistent input length. We evaluated performance using three standard information retrieval metrics: Precision@k, Recall@k, and nDCG@k, where k takes values of 1, 5, 10, 20, 50, and 100. For each dataset, we randomly sampled queries and evaluated retrieval performance against the entire corpus.

\subsection{Results on German Data}

The fine-tuned model showed clear improvements over the baseline on German legal data. The baseline model achieved a Recall@100 of 0.5066, meaning it retrieved approximately 50.7 percent of all relevant documents in the top 100 results. The fine-tuned model improved this to 0.7361, representing a substantial increase of over 23 percentage points. Precision@1 improved from 0.1500 to 0.1615, indicating better accuracy in the top result. The nDCG@100 metric increased from 0.2863 to 0.3748, showing improved ranking quality. These results demonstrate that fine-tuning on German legal data significantly improves retrieval performance on the same jurisdiction.

\subsection{Results on Austrian Data}

The Austrian dataset showed very poor performance for both baseline and fine-tuned models. The baseline model achieved a Recall@100 of only 0.0460, meaning it retrieved less than 5 percent of relevant documents in the top 100 results. The fine-tuned model showed minimal improvement, with Recall@100 reaching 0.0460, essentially unchanged. Precision@1 remained at 0.0000 for both models, indicating that the top-ranked result was never relevant. The nDCG@100 values were 0.0110 for baseline and 0.0112 for fine-tuned, both extremely low.

These poor results suggest that the model struggles significantly with Austrian legal documents. Several factors may contribute to this. The Austrian dataset is synthetic, with queries generated using large language models rather than being naturally occurring legal queries. This may introduce distributional differences that the model cannot handle. Additionally, while Austrian and German legal systems share similarities, there may be sufficient differences in terminology, document structure, or legal concepts that prevent effective transfer.

\subsection{Results on Chinese Data}

The Chinese dataset showed interesting results. The baseline model already achieved strong performance, with Recall@100 of 0.9299 and Precision@1 of 0.8239. The fine-tuned model showed very similar performance, with Recall@100 of 0.9118 and Precision@1 of 0.8302. The nDCG@100 values were 0.8579 for baseline and 0.8407 for fine-tuned, with the fine-tuned model actually showing a slight decrease.

These results indicate that fine-tuning on German legal data does not improve performance on Chinese legal documents. The baseline model already performs well, likely because the pre-trained Qwen3-Embedding-8B model has strong multilingual capabilities and was trained on diverse text including Chinese. The fine-tuning on German data appears to provide no additional benefit for Chinese retrieval, and may even slightly degrade performance by shifting the embedding space toward German legal concepts.

\section{Conclusion}

Our experiments reveal mixed results regarding cross-jurisdictional transfer of fine-tuned legal embedding models. Fine-tuning on German legal data significantly improves performance on German documents, demonstrating that domain-specific fine-tuning is effective when applied to the same jurisdiction. However, transfer to other jurisdictions shows limited success.

The Austrian results show that fine-tuning provides essentially no benefit, with both baseline and fine-tuned models performing poorly. This suggests that the differences between German and Austrian legal systems, combined with the synthetic nature of the Austrian dataset, prevent effective knowledge transfer. The Chinese results show that fine-tuning is unnecessary, as the baseline model already achieves strong performance, likely due to the model's inherent multilingual capabilities.

These findings have important implications for legal information retrieval systems. Fine-tuning on one legal system does not guarantee improved performance on others. The effectiveness of transfer depends on factors such as language similarity, legal system similarity, and dataset quality. For jurisdictions with similar languages and legal frameworks, some transfer may occur, but it cannot be assumed. For completely different languages and legal systems, the pre-trained multilingual capabilities of foundation models may be sufficient without fine-tuning.

Future work could investigate more sophisticated transfer learning approaches, such as multi-task learning across jurisdictions or domain adaptation techniques. Additionally, analyzing the specific factors that enable or prevent transfer would provide deeper insights into cross-jurisdictional legal information retrieval.

\newpage
\printbibliography 

\end{document}
